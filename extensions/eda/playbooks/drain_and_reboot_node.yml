---
- name: "Playbook reacting to alerts nfs-stale OR node-health-check"
  hosts: localhost
  gather_facts: false
  vars_prompt:
    - name: payload
      prompt: ""
      private: false

  pre_tasks:
    - name: Set fact node_name based on existing label from alert
      set_fact:
        problematic_node: "{{ payload.labels[item] }}"
      loop:
        - node
        - instance
      when: payload.labels is defined and item in payload.labels

    - name: "Include vars file for the target environment"
      include_vars: "{{ payload.labels.env }}.yml"

  tasks:
    - name: "Log in to the correct OpenShift cluster"
      shell: "oc login --insecure-skip-tls-verify=true --token={{ openshift_token }} --server={{ openshift_api }}"

##### Just for the workshop
    - name: "Delete Gitea pod only"
      shell: "oc -n gitea delete $(oc get po -n gitea -l app=gitea --no-headers -o name)"

    - name: "Print alert label"
      shell: "echo {{ problematic_node }}"
#### in our lab we rely on SNO so we cannot really reboot the node......
 
    #- name: "Validation: Donâ€™t reboot node if you have more then 1 NotReady nodes or 3 alert simultaniously"
    #  shell: |
    #    numOfAlertsSimultaniuously=`oc -n openshift-monitoring exec -it $(oc -n openshift-monitoring get pod -l alertmanager=main -o jsonpath='{.items[0].metadata.name}') -- curl -s http://localhost:9093/api/v2/alerts | jq '.[] | select(.labels.alertname=="detect-nfs-stale" or .labels.alertname=="node-health-check") | .fingerprint' | sort -u | wc -l` 
    #    numOfNotReadyNodes=`oc get nodes | grep -E 'NotReady|SchedulingDisabled' | wc -l`
    #    if [ "$numOfAlertsSimultaniuously" -gt 3 ] || [ "$numOfNotReadyNodes" -gt 1 ] ; then
    #      exit 1;
    #    else
    #      exit 0;
    #    fi
    #  register: validations_status
    #  until: validations_status.rc == 0
    #  retries: 10
    #  delay: 600

    #- name: "Drain node of its pods"
    #  shell: "oc adm drain {{ problematic_node }} --force --delete-emptydir-data --ignore-daemonsets"
    #  register: drain_status

    #- name: "Create rebooter pod manifest from template"
    #  template:
    #    src: ../../templates/reboot_node_pod.yaml.j2
    #    dest: "/tmp/reboot_node_pod-{{ problematic_node }}.yaml"

    #- name: "Apply the rebooter pod manifest to trigger the node reboot"
    #  shell: "oc apply -f /tmp/reboot_node_pod-{{ problematic_node }}.yaml"